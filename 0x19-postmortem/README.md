üßÆ BOTH MANDATORY OR ADVCANCED QUESTION ANSWERED HERE  üßë

:Postmortem: Resolving the Dbazz Website Downtime üíØ üßë‚Äç‚öñÔ∏è

It was a sudden and alarming situation when our highly popular website, Dbazz Website, which is widely used by Africans in the Diaspora to transfer money in any currency to their families back home without any hidden charges or costs, went DOWN!

The panic was real because we have close to 50 million individuals who visit our website daily to perform various transactions, and we could not afford to lose our dedicated customers to competition. As one of Dbazz's System DevOps Engineers, it was my primary responsibility to debug and resolve such issues when they arise, which happens occasionally.

To solve this, I performed all the required system administration and configuration management checks by troubleshooting and debugging. This was done to identify and resolve issues related to system performance and configuration to understand the root cause of the problem.

During the debugging process, I spotted the bug: Something was preventing Dbazz's Ubuntu container's Nginx installation from listening on port 80. This is highly problematic because if Nginx is not listening on port 80, it can lead to the servers being inaccessible from the internet. Port 80 is the default port for HTTP traffic, and if Nginx is not properly configured to listen on this port, incoming web requests won't be directed to the appropriate backend servers, resulting in a downtime situation, which was the case with the Dbazz Website.

When I debugged, the observed configuration was as follows:

server {
    listen 8080;  // Incorrect port number
    server_name 197767-web-01.ubuntu.com;

    location / {
        proxy_pass http://backend_servers;
    }
}

upstream backend_servers {
    server 192.168.1.10;
    server 192.168.1.11;
}


This configuration was obviously incorrect for Dbazz's Ubuntu container's Nginx installation, which was supposed to be listening on port 80. Instead, it was listening on port 8080.

The correct configuration should be:


server {
    listen 80;
    server_name 197767-web-01.ubuntu.com;

    location / {
        proxy_pass http://backend_servers;
    }
}

upstream backend_servers {
    server 192.168.1.10;
    server 192.168.1.11;
}

This detailed incident report (postmortem) outlines the troubleshooting and debugging process that took place to resolve the Dbazz Website downtime.

Changes:
1. Changed "ever so often" to "occasionally" to better reflect the frequency.
2. Added "server_name" directive in the Nginx configuration blocks, which was missing.
3. Corrected minor grammar and phrasing issues.

Here's the updated version with some corrections:

Issue Summary: The outage occurred on August 11, 2023, from 10:00 AM to 11:30 AM (UTC). During this period, the Nginx service was unavailable on the Ubuntu server, resulting in a complete disruption of web services. Users experienced slow page loading and were unable to access the Dbazz website hosted on the server. Approximately 85% of users were affected by the downtime.

Root Cause and Resolution: The root cause of the issue was a misconfigured Nginx server configuration. The Nginx process was not listening on port 80 due to an incorrect port specification in the configuration file. As a result, incoming HTTP requests were not being properly handled.

To resolve the issue, the following steps were taken:
- Accessed the Nginx configuration file located at /etc/nginx/nginx.conf.
- Verified the listen directive in the default server block, which was set to listen 8080 instead of listen 80.
- Corrected the listen directive to listen 80.
- Restarted the Nginx service using the command sudo systemctl restart nginx.

Corrective and Preventative Measures: To prevent similar issues in the future, the following actions will be taken:

- Implement automated configuration validation scripts to detect misconfigured settings.
- Establish stricter code review processes for configuration changes to critical services.
- Set up regular internal training sessions to educate team members about common misconfigurations.
- Create a comprehensive playbook for debugging and resolving service disruptions, including common pitfalls and their solutions.

In conclusion, the outage was caused by a misconfigured Nginx server that prevented it from listening on port 80, resulting in downtime for users. The incident was resolved by correcting the misconfiguration and restarting the Nginx service. Moving forward, more preventive measures will be implemented to detect and address similar issues promptly, ensuring better service availability and user experience.

At Dbazz, we pride ourselves on giving the best experience to our customers, always.

Thank you for taking the time to read this incident report.

Changes:
- Corrected the command to restart Nginx service (sudo systemctl restart nginx instead of sudo service nginx restart).
- Fixed a grammatical error ("on giving" changed to "on giving").
- Removed the unnecessary closing statement about staying tuned for more insights, as it doesn't fit the tone of an incident report.
  # Postmortem: The Great Nginx Outage

## Issue Summary

On August 11, 2023, our website experienced a significant outage from 10:00 AM to 11:30 AM UTC. During this period, the Nginx service, which is the backbone of our web server infrastructure, went down, resulting in a complete disruption of our services. Users reported slow page loading times and were ultimately unable to access our website. Approximately 85% of our user base was affected by this downtime, causing widespread frustration and potential loss of trust in our platform.

The root cause of the issue was a misconfigured Nginx server configuration, which prevented the service from listening on the default HTTP port (80).

## Timeline

- 10:05 AM UTC: The issue was detected by our monitoring system, which raised an alert regarding the unresponsiveness of our web servers.
- 10:10 AM UTC: Our on-call DevOps engineer acknowledged the alert and initiated an investigation.
- 10:15 AM UTC: The initial assumption was that the issue might be related to a sudden spike in web traffic, leading to resource exhaustion.
- 10:30 AM UTC: After ruling out the traffic spike hypothesis, the investigation shifted towards potential network issues and firewall configurations.
- 10:45 AM UTC: Escalation to the broader DevOps team and senior engineers took place as the root cause remained elusive.
- 11:00 AM UTC: A thorough review of the Nginx configuration files revealed the culprit ‚Äì the service was misconfigured to listen on the wrong port.
- 11:15 AM UTC: The issue was resolved by correcting the Nginx configuration and restarting the service.
- 11:30 AM UTC: Monitoring confirmed that the web servers were back online, and user traffic was gradually restored.

## Root Cause and Resolution

The root cause of the outage was a misconfigured Nginx server configuration file. Instead of listening on the default HTTP port (80), the configuration had been mistakenly set to listen on port 8080. As a result, incoming HTTP requests were not being properly handled, leading to the website becoming inaccessible.

To resolve the issue, the following steps were taken:

1. The Nginx configuration file (`/etc/nginx/nginx.conf`) was accessed and inspected.
2. The `listen` directive in the default server block was identified as `listen 8080` instead of `listen 80`.
3. The incorrect port specification was corrected by changing `listen 8080` to `listen 80`.
4. The Nginx service was restarted using the command `sudo systemctl restart nginx`.
5. Monitoring confirmed that the service was now listening on the correct port, and web traffic was successfully being handled.

## Corrective and Preventative Measures

While the issue was resolved promptly, we recognize the need for improvements to prevent similar incidents from occurring in the future. To this end, the following corrective and preventative measures will be implemented:

- Automated configuration validation scripts will be developed and integrated into our deployment pipeline to detect misconfigured settings before they reach production environments.
- Stricter code review processes will be established for configuration changes to critical services, ensuring multiple pairs of eyes review and approve changes before deployment.
- Regular internal training sessions will be conducted to educate team members about common misconfigurations, best practices, and the importance of thorough testing.
- A comprehensive playbook for debugging and resolving service disruptions will be created, including common pitfalls, their solutions, and clear escalation paths.

By implementing these measures, we aim to enhance our overall system reliability, reduce the likelihood of similar incidents, and foster a culture of continuous improvement within our DevOps team.

## Closing Thoughts

In the ever-evolving world of technology, even the smallest misconfiguration can have significant consequences. While we deeply regret the inconvenience caused to our users, this incident has served as a valuable learning experience. We remain committed to providing the best possible service and user experience, and we will continue to prioritize system stability and resilience.

